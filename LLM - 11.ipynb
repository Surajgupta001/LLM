{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6993e3",
   "metadata": {},
   "source": [
    "# POSITIONAL EMBEDDING (ENCODING WORDS POSITION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dace475",
   "metadata": {},
   "source": [
    "Previously, we focused on very small embedding sizes in the chapter for illustration purposes.\n",
    "\n",
    "We now consider more realistic and useful embedding sizes, and encode the input token into a 256-dimensional vector representation.\n",
    "\n",
    "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions for the largest model), but still reasonable for demonstration.\n",
    "\n",
    "Further move, we assume that the token IDs were cratede by the BPE tokenizer, that we implemented earlier, which has a vocabulary size of 50,257 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21a4163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7aa7c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8ffe64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5775281",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257  # Vocabulary size of the tokenizer\n",
    "output_dim = 256   # Dimensionality of the embedding vector\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed581f3",
   "metadata": {},
   "source": [
    "using the token_embedding_layer above, if we sample data from the data loader, we embed each token in each batch into a batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923bc98",
   "metadata": {},
   "source": [
    "Let's instantiate the data loader (Data sampling with a sliding window). first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59e148ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20480\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9edbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4  # Maximum sequence length\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False,\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, target = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a60bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Input shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print('Token IDs:\\n', inputs)\n",
    "print('\\nInput shape:\\n', inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448ca0d",
   "metadata": {},
   "source": [
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch consists of 8 text samples with 4 tokens each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08326496",
   "metadata": {},
   "source": [
    "Let's now use the embedding layer to embed these token IDs into 256-dimensional\n",
    "vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f5778cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Embeddings shape:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print('\\nToken Embeddings shape:\\n', token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb629c4",
   "metadata": {},
   "source": [
    "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now\n",
    "embedded as a 256-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d64e5",
   "metadata": {},
   "source": [
    "For a GPT model's absolute embedding approach, we just need to create another\n",
    "embedding layer that has the same dimension as the token_embedding_layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7795df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length  # Context length for the GPT model\n",
    "position_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77adeb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Position Embeddings shape:\n",
      " torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "position_embeddings = position_embedding_layer(torch.arange(max_length))\n",
    "print('\\nPosition Embeddings shape:\\n', position_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080a31c",
   "metadata": {},
   "source": [
    "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
    "placeholder vector torch.arange(context_length), which contains a sequence of\n",
    "numbers 0, 1, ..., up to the maximum input length âˆ’ 1. \n",
    "\n",
    "The context_length is a variable\n",
    "that represents the supported input size of the LLM. \n",
    "\n",
    "Here, we choose it similar to the\n",
    "maximum length of the input text. \n",
    "\n",
    "In practice, input text can be longer than the supported\n",
    "context length, in which case we have to truncate the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbf079",
   "metadata": {},
   "source": [
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
    "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
    "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
    "each of the 8 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03233ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Embeddings shape:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + position_embeddings\n",
    "print('\\nInput Embeddings shape:\\n', input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
