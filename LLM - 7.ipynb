{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d028d5b",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092cf1c",
   "metadata": {},
   "source": [
    "# Reading in a short story as text sample into python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f77e9",
   "metadata": {},
   "source": [
    "## Step -1 : Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6565ce",
   "metadata": {},
   "source": [
    "The Print command prints the total numbers of character followed by the first 1000 characters of the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f444a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text: 20480\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "print(\"Total number of characters in the text:\", len(raw_text))\n",
    "print(raw_text[:99])  # Displaying the first 999 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67d1ae",
   "metadata": {},
   "source": [
    "**Our Goal is to tokenize this 20,480-character short into individual words and special cahracters, that we can the turn into embeddings for LLM training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d80f6e",
   "metadata": {},
   "source": [
    "Note that it's common to process of articles and hundreds of thousands of books -- many gigabytes of text -- when working with LLMs. However, for educational purposes, is's sufficient to work with smaller text samples like single book to illustrate the main ideasto you possible to run it in reasonable time on sonsumer hardwere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b52df7",
   "metadata": {},
   "source": [
    "**How can be best spli this text to obtain a list of tokens? For this, we go on a small excursion and use python's regualr expression module `re` to split the text into tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5388ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello,', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.', ' ', \"Let's\", ' ', 'see', ' ', 'how', ' ', 'it', ' ', 'works.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world! This is a test. Let's see how it works.\"\n",
    "result = re.split(r'(\\s)', text) # \\s matches any whitespace character (space, tab, newline, etc.)\n",
    "print(\"Tokens:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4572693",
   "metadata": {},
   "source": [
    "**Let's modify the regular expression splits on whitespaces (\\s) and commas (,), periods (.), and apostrophes (') to create a list of tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d6297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with custom regex: ['Hello', ',', '', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '', ' ', \"Let's\", ' ', 'see', ' ', 'how', ' ', 'it', ' ', 'works', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)  # Splitting on whitespace, commas, periods, and apostrophes\n",
    "print(\"Tokens with custom regex:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af8607",
   "metadata": {},
   "source": [
    "We can see that the words and punctuation characters are now seperat ist entries just as we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f2403",
   "metadata": {},
   "source": [
    "**A small remaining issue is that the list still includes whitespaces characters. Optinally, we can remove these rebundant characters safely as follows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be99fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without whitespace: ['Hello', ',', 'world!', 'This', 'is', 'a', 'test', '.', \"Let's\", 'see', 'how', 'it', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(\"Tokens without whitespace:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f923469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with punctuation and whitespace removed: ['Hello', ',', 'world', '!', 'Is', 'this', '--', 'a', 'test', '?', 'Yes', ',', 'it', 'is', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! Is this -- a test? Yes, it is.\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)  # Including punctuation and whitespace\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(\"Tokens with punctuation and whitespace removed:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30be0468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed tokens: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(\"Preprocessed tokens:\", preprocessed[:30])  # Displaying the first 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63120992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4690\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of tokens:\", len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f1ff5",
   "metadata": {},
   "source": [
    "## Step -2 : Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f67bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00941a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33cc361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "830c8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int.get(token) for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace space before the specified punctuations\n",
    "        text = re.sub(r'\\s([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b20c03b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [1, 56, 2, 850, 988, 602, 533, None, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he panited, you know,\"\n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(\"Encoded IDs:\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e1f86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out None values before decoding to avoid KeyError\n",
    "filtered_ids = [i for i in ids if i is not None]\n",
    "tokenizer.decode(filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37fddd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 5, 355, 1126, 628, 975, 10]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadf5d5",
   "metadata": {},
   "source": [
    "## Adding Special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23be559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token = sorted(list(set(preprocessed)))\n",
    "all_token.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f641c0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "881a06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17bbdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            token if token in self.str_to_int\n",
    "            else \"<|unk|>\" for token in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace space before the specified punctuations\n",
    "        text = re.sub(r'\\s([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78692750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sulight terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sulight terraces of the palace.\"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6e2612e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<|unk|>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m, in \u001b[0;36mSimpleTokenizerV2.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m ]\n\u001b[0;32m     11\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     token \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|unk|>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed\n\u001b[0;32m     14\u001b[0m ]\n\u001b[1;32m---> 15\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m ]\n\u001b[0;32m     11\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     token \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|unk|>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed\n\u001b[0;32m     14\u001b[0m ]\n\u001b[1;32m---> 15\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: '<|unk|>'"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
